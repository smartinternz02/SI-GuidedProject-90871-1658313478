# -*- coding: utf-8 -*-
"""externship

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ccBFOh5yKi1Y39xvzc7kgbYzUEaan-No
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import warnings
#warnings.filterwarnings (" ignore')
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import pickle
from sklearn.metrics import classification_report, confusion_matrix

#Reading the csv and printing its shape
df = pd.read_csv(r'/content/emp_promotion.csv')
df.head()

df.info()

# Data 2s mbalanced
plt.figure(figsize= (10, 4))
plt.subplot (121)
sns.countplot(df['is_promoted'])
plt.subplot(122)
df["is_promoted"]. value_counts().plot (kind="pie", autopct = "%.2f%%",shadow=True)
plt.show()

plt.figure(figsize=(16, 10))
plt.subplot(231)
plt.axis("off")
plt.title( "KPIs_met >80%")
df["KPIs_met >80%"]. value_counts ().plot (kind="pie", shadow=True, autopct = "%.2f%%")
plt.subplot(232)
plt.axis("off")
plt.title("awards_won?")
df['awards_won?'].value_counts(). plot (kind='pie', shadow=True, autopct= "%.2f%%")
plt.subplot (233)
plt.axis("off")
plt.title("previous_year_rating")
df['previous_year_rating'].value_counts().plot(kind="pie", shadow=True, autopct="%.2f%%")
plt.show()

plt.figure(figsize=(14,6))
plt.subplot(121)
sns.boxplot(df['length_of_service'],color='g')
plt.subplot(122)
sns.boxplot(df["avg_training_score"],color='g')

plt.figure(figsize= (20, 6))
sns.barplot (df[ 'avg_training_score'], df[ 'previous_year_rating'], df['is_promoted'])

df.describe(include="all")

df.head()

df.drop(["employee_id", "gender", "region", "recruitment_channel"],axis= 1,inplace=True)

df.head()

df.isnull().sum()

#Replacing nan with mode
print (df["education"].value_counts())
df['education'] = df["education"].fillna(df['education'].mode()[0])

# Replacing nan with mode
print(df['previous_year_rating'].value_counts())
df['previous_year_rating'] = df["previous_year_rating"].fillna(df["previous_year_rating"].mode()[0])

# Findng the empLoyee who got promoted even in poor performance. lt affect model performance.
negative=df[(df["KPIs_met >80%"]==0) & (df["awards_won?"]==0) & (df['previous_year_rating']==1.0) &
         (df["is_promoted"]==1) & (df['avg_training_score']<60)]
negative

#Removing negative data
df.drop(index=[31860,51374], inplace=True)

#Handling outliers
q1=np.quantile(df["length_of_service"],0.25)
q3=np.quantile(df["length_of_service"],0.75)
IQR = q3-q1
upperBound = (1.5*IQR)+q3
lowerBound = (1.5*IQR)-q1
print("q1 :",q1)
print("q3 :",q3)
print("IQR: ",IQR)
print("Upper Bound : ",upperBound)
print('LOwer Bound : ',lowerBound)
print("Skewed data :",len(df[df["length_of_service"]>upperBound]))

pd.crosstab([df['length_of_service']>upperBound],df["is_promoted"])

# Capping
df['length_of_service']=[upperBound if x>upperBound else x for x in df['length_of_service']]

# Feature mapping is done on education column
df['education']=df['education'].replace(("Below Secondary","Bachelor's","Master's & above"), (1,2,3))

lb = LabelEncoder()
df['department']=lb.fit_transform(df['department'])

# Splitting data and resampling it

x = df.drop('is_promoted',axis=1)
y = df["is_promoted"]
print(x. shape)
print(y.shape)

from imblearn.over_sampling import SMOTE
sm =SMOTE()
x_resample, y_resample = sm.fit_resample (x,y)

x_train, x_test, y_train, y_test=train_test_split(x_resample,y_resample,test_size=0.3, random_state=10)

print("Shape of x_train {}".format(x_train.shape))
print("shape of y_train {}".format(y_train.shape))
print("Shape of x_test {}".format(x_test.shape))
print('Shape of y_test {}'.format (y_test.shape))

def decisionTree(x_train, x_test, y_train, y_test):

    dt=DecisionTreeClassifier()
    dt.fit(x_train,y_train)
    yPred= dt.predict(x_test)
    print('***DecisionTreeClassifier***')
    print("Confusion matrix")
    print(confusion_matrix(y_test,yPred))
    print("Classification report")
    print(classification_report (y_test, yPred))

def randomForest(x_train, x_test, y_train, y_test):
    rf = RandomForestClassifier()
    rf.fit(x_train,y_train)
    yPred = rf.predict(x_test)
    print("***RandomForestClassifier***")
    print("Confusion matrix")
    print(confusion_matrix(y_test,yPred))
    print("Classification report")
    print(classification_report (y_test,yPred))

def KNN(x_train, x_test, y_train, y_test):
   knn = KNeighborsClassifier()
   knn.fit(x_train, y_train)
   yPred= knn.predict(x_test)
   print("***kNeighborsClassifiers***")
   print("Confusion matrix")
   print(confusion_matrix(y_test,yPred))
   print("Classification report")
   print(classification_report(y_test,yPred))

def xgboost(x_train, x_test, y_train, y_test):
  xg=GradientBoostingClassifier()
  xg.fit(x_train,y_train)
  yPred = xg.predict(x_test)
  print("***GradientBoostingClassifier***")
  print("Confusion matrix")
  print(confusion_matrix(y_test,yPred))
  print('Classification report')
  print(classification_report(y_test,yPred))

def compareModel(x_train, x_test, y_train, y_test):
   decisionTree(x_train, x_test, y_train, y_test)
   print('-'*100)
   randomForest(x_train, x_test, y_train, y_test)
   print("-"*100)
   KNN(x_train, x_test, y_train, y_test)
   print('-'*100)
   xgboost(x_train, x_test, y_train, y_test)

compareModel(x_train, x_test, y_train, y_test)

# Random forest model is selected

rf=RandomForestClassifier()
rf.fit(x_train, y_train)
yPred = rf.predict(x_test)

cv=cross_val_score(rf,x_resample, y_resample, cv=5)
np.mean(cv)

pickle.dump(rf,open('model.pkl','wb'))